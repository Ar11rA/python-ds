{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "568454\n393933\n69 % of data cleaned!\n250962\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# reading\n",
    "df = pd.read_csv('../amazon-fine-food-reviews/Reviews.csv')\n",
    "print(df.Id.count())\n",
    "\n",
    "# cleaning\n",
    "sorted_df = df.sort_values('ProductId', axis=0, ascending=True,inplace=False)\n",
    "dedupped_df = df.drop_duplicates(subset={\"UserId\", \"ProfileName\", \"Time\",\"Text\"},keep=\"first\", inplace=False)\n",
    "\n",
    "print(dedupped_df.Id.count())\n",
    "print(\"{} % of data cleaned!\".format(int(dedupped_df.Id.count()/df.Id.count() * 100) ))\n",
    "# filtering 5 star products\n",
    "five_rating_df = dedupped_df[dedupped_df['Score'] == 5]\n",
    "\n",
    "print(five_rating_df.Id.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'\n 'Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.'\n \"This saltwater taffy had great flavors and was very soft and chewy.  Each candy was individually wrapped well.  None of the candies were stuck together, which did happen in the expensive version, Fralinger's.  Would highly recommend this candy!  I served it at a beach-themed party and everyone loved it!\"]\n['all', 'and', 'appreciates', 'assortment', 'at', 'be', 'beach', 'better', 'bought', 'candies', 'candy', 'canned', 'chewy', 'deal', 'delivery', 'did', 'dog', 'each', 'everyone', 'expensive', 'finicky', 'flavors', 'food', 'found', 'fralinger', 'good', 'great', 'had', 'happen', 'have', 'highly', 'if', 'in', 'individually', 'is', 'it', 'labrador', 'like', 'looks', 'loved', 'lover', 'meat', 'more', 'most', 'my', 'none', 'of', 'party', 'price', 'processed', 'product', 'products', 'quality', 'quick', 'recommend', 'saltwater', 'served', 'several', 'she', 'smells', 'soft', 'stew', 'stuck', 'taffy', 'than', 'the', 'them', 'themed', 'there', 'this', 'to', 'together', 'version', 'very', 'vitality', 'was', 'well', 'were', 'which', 'wide', 'would', 'wrapped', 'your', 'yummy']\n[[1 3 1 0 0 1 0 2 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 2 0 0 0 0 1 1\n  1 1 1 0 0 1 1 1 1 0 2 0 0 1 2 1 1 0 0 0 0 1 1 1 0 1 0 0 2 2 1 0 0 1 1 0\n  0 0 1 0 0 0 0 0 0 0 0 0]\n [0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 1 0 0 1 0\n  0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 3 0 0 0 0 1 1 0 0\n  0 1 0 2 0 0 0 1 0 0 1 1]\n [0 3 0 0 1 0 1 0 0 1 2 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 2\n  0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 2 0 1 0 2 0 1\n  1 1 0 2 1 1 1 0 1 1 0 0]]\n"
    }
   ],
   "source": [
    "# Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vector_array = vectorizer.fit_transform(five_rating_df.head(3).Text.values)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(vector_array.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "amaz\namaz\n['i hve bugh everl f he vl cnne g f pruc n hve fun he ll  be f g qul the pruc lk re lke  ew hn  prcee e n  ell beer m lbrr  fnck n he pprece h pruc beer hn  ', 'gre ff   gre prce  there w  we ren f u ff  delver w ver quck  if ur  ff lver h   el', \"th lwer ff h gre flvr n w ver f n chew  ech cn w nvull wrppe well  nne f he cne were uck geher whch  hppen n he expenve vern frlnger'  wul hghl recen h cn  i erve    bech-hee pr n everne lve \"]\n['be', 'bech', 'beer', 'bugh', 'chew', 'cn', 'cne', 'cnne', 'delver', 'ech', 'el', 'ell', 'erve', 'everl', 'everne', 'ew', 'expenve', 'ff', 'flvr', 'fnck', 'frlnger', 'fun', 'geher', 'gre', 'he', 'hee', 'hghl', 'hn', 'hppen', 'hve', 'if', 'lbrr', 'lk', 'lke', 'll', 'lve', 'lver', 'lwer', 'nne', 'nvull', 'pprece', 'pr', 'prce', 'prcee', 'pruc', 'quck', 'qul', 're', 'recen', 'ren', 'th', 'the', 'there', 'uck', 'ur', 'ver', 'vern', 'vl', 'we', 'well', 'were', 'whch', 'wrppe', 'wul']\n[[0.15040469 0.         0.30080938 0.15040469 0.         0.\n  0.         0.15040469 0.         0.         0.         0.15040469\n  0.         0.15040469 0.         0.15040469 0.         0.\n  0.         0.15040469 0.         0.15040469 0.         0.\n  0.34315993 0.         0.         0.30080938 0.         0.30080938\n  0.         0.15040469 0.15040469 0.15040469 0.15040469 0.\n  0.         0.         0.         0.         0.15040469 0.\n  0.         0.15040469 0.45121407 0.         0.15040469 0.15040469\n  0.         0.         0.         0.15040469 0.         0.\n  0.         0.         0.         0.15040469 0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.23506588 0.         0.23506588 0.\n  0.         0.         0.         0.         0.         0.53632099\n  0.         0.         0.         0.         0.         0.35754732\n  0.         0.         0.         0.         0.         0.\n  0.23506588 0.         0.         0.         0.         0.\n  0.23506588 0.         0.         0.         0.         0.\n  0.23506588 0.         0.         0.23506588 0.         0.\n  0.         0.23506588 0.         0.         0.23506588 0.\n  0.23506588 0.17877366 0.         0.         0.23506588 0.\n  0.         0.         0.         0.        ]\n [0.         0.16891314 0.         0.         0.16891314 0.33782629\n  0.16891314 0.         0.         0.16891314 0.         0.\n  0.16891314 0.         0.16891314 0.         0.16891314 0.1284628\n  0.16891314 0.         0.16891314 0.         0.16891314 0.1284628\n  0.2569256  0.16891314 0.16891314 0.         0.16891314 0.\n  0.         0.         0.         0.         0.         0.16891314\n  0.         0.16891314 0.16891314 0.16891314 0.         0.16891314\n  0.         0.         0.         0.         0.         0.\n  0.16891314 0.         0.16891314 0.         0.         0.16891314\n  0.         0.1284628  0.16891314 0.         0.         0.16891314\n  0.16891314 0.16891314 0.16891314 0.16891314]]\n"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "input_df = five_rating_df.head(3).Text.values\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "print(sno.stem('amaze'))\n",
    "print(sno.stem('amazing'))\n",
    "\n",
    "def clean_html(s):\n",
    "    a=re.sub('[|>.*?|\\.*|?.*?]',\"\",s)\n",
    "    return a\n",
    "def clean_punc(s):\n",
    "    a=re.sub('[.|,|!,|)|(|/|\\|”|\\’|#|@|$|-|%|]',\"\",s)\n",
    "    return a\n",
    "def clean_word(s):\n",
    "    out = \"\"\n",
    "    for w in s:\n",
    "        if  w not in stop:        \n",
    "            cleaned_word = clean_punc(w)\n",
    "            stemmed_word = sno.stem(cleaned_word)\n",
    "            out = out + stemmed_word\n",
    "    return out\n",
    "\n",
    "html_cleaned_sentences = list(map(lambda x:clean_html(x), input_df))\n",
    "stemmed_word_sentences =  list(map(clean_word, html_cleaned_sentences))\n",
    "print(stemmed_word_sentences)\n",
    "vector_array = vectorizer.fit_transform(stemmed_word_sentences)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(vector_array.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['all', 'and', 'appreciates', 'assortment', 'at', 'be', 'beach', 'better', 'bought', 'candies', 'candy', 'canned', 'chewy', 'deal', 'delivery', 'did', 'dog', 'each', 'everyone', 'expensive', 'finicky', 'flavors', 'food', 'found', 'fralinger', 'good', 'great', 'had', 'happen', 'have', 'highly', 'if', 'in', 'individually', 'is', 'it', 'labrador', 'like', 'looks', 'loved', 'lover', 'meat', 'more', 'most', 'my', 'none', 'of', 'party', 'price', 'processed', 'product', 'products', 'quality', 'quick', 'recommend', 'saltwater', 'served', 'several', 'she', 'smells', 'soft', 'stew', 'stuck', 'taffy', 'than', 'the', 'them', 'themed', 'there', 'this', 'to', 'together', 'version', 'very', 'vitality', 'was', 'well', 'were', 'which', 'wide', 'would', 'wrapped', 'your', 'yummy']\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector_array = vectorizer.fit_transform(five_rating_df.head(3).Text.values)\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['this', 'pizza', 'is', 'amazing', 'and', 'bad', 'awesome']\n[('pizza', 0.49744996428489685), ('this', -0.0525684654712677), ('and', -0.16359305381774902), ('amazing', -0.18564680218696594), ('is', -0.6302595138549805), ('awesome', -0.6873972415924072)]\n"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "# model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# print(model.wv['nice'])\n",
    "# print(model.wv.similarity('nice', 'good'))\n",
    "# print(model.wv.most_similar('good'))\n",
    "\n",
    "sentences = [['this', 'pizza', 'is', 'amazing', 'and', 'delicious'],\n",
    "\t\t\t['this', 'pizza', 'is', 'bad', 'and', 'waste'],\n",
    "\t\t\t['this', 'pizza', 'is', 'not', 'amazing', 'and', 'bad'],\n",
    "\t\t\t['this', 'pizza', 'is', 'tasty', 'and', 'awesome'],\n",
    "\t\t\t['this', 'pizza', 'is', 'amazing', 'and', 'awesome']]\n",
    "w2v_model=Word2Vec(sentences ,min_count=2,size=3, workers=4)\n",
    "print(list(w2v_model.wv.vocab))\n",
    "print(w2v_model.wv.most_similar('bad'))\n",
    "\n"
   ]
  }
 ]
}